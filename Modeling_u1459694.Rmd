---
title: "Modeling"
author: "Ahsan Ahmad"
date: "2024-10-29"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)

```

# Introduction

Swire Coca-Cola faces a significant production shortfall across its six facilities, producing only 94.4% of its intended 192 million cases annually due to unplanned machinery breakdowns. These unexpected failures result in approximately $60 million in lost productivity each year. The current reactive maintenance process, reliant on an Internal Warehouse Controller (IWC) system that issues work orders only after breakdowns occur, creates delays in repairs and extends downtime.

To address this issue, a predictive maintenance solution is proposed through Survival Analysis. By forecasting when machinery failures are likely to happen, Swire Coca-Cola can proactively schedule maintenance, minimize unplanned downtimes, and ensure optimal production efficiency.

This notebook is created after doing a reiterative process of evaluating data and only the final results are published that answer the following questions in the data story:

1. What are the distributions of the important variables in the dataset?
2. What are the basic summary statistics (mean, median, standard deviation, etc.) for each important predictor?
3. How to select a few most important and relevant predictors from the dataset?
4. Are there any missing values in the dataset? What method is used to clean the Data and remove them?
5. What are the relationships between different variables in the dataset? Are there any strong correlations that suggests potential predictive relationships?
6. How can the data be visualized to better understand its characteristics? e.g. creating different plots using ggplot like histograms, scatter plots, box plots, heatmaps etc.

# Description of the data

Initially a data set of 1,427,264 observations has been given by Swire which contains one identifier column (Order_ID) and 24 predictor variables. In this section we will produce summary statistics of the whole data and try to create insights from it. 

```{r}
# Load Data and packages
library(tidyverse) #load libraries for project
library(dplyr) 
library(ggplot2)
library(tidyr)
library(rpart)
library(rpart.plot)
library(skimr)
library(janitor)
library(corrplot)
library(lubridate)
library(survival)
library(survminer)
library(readr)
library(VIM)

data <- read.csv("IWC_Work_Orders_Extract.csv")

# Checking the data to look for Length, Min, Max, Mean, Outliers etc

str(data)

summary(data)

# Displaying the first few rows of data
head(data)

```


# Data Cleaning and Preprocessing

## Handling Missing Data

```{r}
# NEW CODE
# Replace blank strings with NA in character columns only
data <- data %>%
  mutate(across(where(is.character), ~na_if(.x, "")))

# Calculate count of NA and blank values for each column
missing_summary <- data %>%
  summarise(across(everything(), 
                   ~sum(is.na(.)), 
                   .names = "NA_count_{col}")) %>%
  bind_rows(data %>%
              summarise(across(where(is.character), 
                               ~sum(. == "", na.rm = TRUE), 
                               .names = "blank_count_{col}")))

# Display the result
print(missing_summary)

# Visualize missing data patterns
aggr_plot <- aggr(data, col = c('navyblue', 'red'), numbers = TRUE, sortVars = TRUE,
                  labels = names(data), cex.axis = .7, gap = 3,
                  ylab = c("Histogram of missing data", "Pattern"))

# Dropping rows with missing values in EQUIPMENT_ID and MAINTENANCE_ITEM
data <- data %>% 
  filter(!is.na(FUNCTIONAL_AREA_NODE_2_MODIFIED))

# Checking the dataset after dropping rows
summary(data)

# Checking for missing values again
na_values <- colSums(is.na(data))
print(na_values)

```

The above numbers show that about 90% of the column "MAINTENANCE_ITEM" contains NA values and about 80% of the column "EQUIPMENT_ID" contains NA values. The high percentage of NA values are probably due to the reason that there are around 60 smaller components for each of the equipment. Therefore, we can't remove these rows nor can we impute any ID numbers for these values. For the sake of this EDA and to prepare data for survival analysis let's assume that the Equipment ID is the same as the preceding one before NA values since it might be possible as one Equipment ID is associated with approximately 60 smaller components.

Non-NA Columns:
ORDER_ID
PLANT_ID
PRODUCTION_LOCATION
EXECUTION_START_DATE
EXECUTION_FINISH_DATE
ACTUAL_START_TIME
ACTUAL_FINISH_TIME
ACTUAL_WORK_IN_MINUTES
MAINTENANCE_ACTIVITY_TYPE


## Changing Column Data Types

```{r}
# Converting Date and Time Columns to type "Date" and Minutes accordingly

data <- data %>% 
  mutate(
    EXECUTION_START_DATE = as.Date(EXECUTION_START_DATE, format = "%Y-%m-%d"),
    EXECUTION_FINISH_DATE = as.Date(EXECUTION_FINISH_DATE, format = "%Y-%m-%d"),
    EQUIP_START_UP_DATE = as.Date(EQUIP_START_UP_DATE, format = "%Y-%m-%d"),
    EQUIP_VALID_FROM = as.Date(EQUIP_VALID_FROM, format = "%Y-%m-%d"),
    EQUIP_VALID_TO = as.Date(EQUIP_VALID_TO, format = "%Y-%m-%d"),
    ACTUAL_START_TIME = hms(ACTUAL_START_TIME),
    ACTUAL_FINISH_TIME = hms(ACTUAL_FINISH_TIME)
  )

# Converting Non-Numeric and non-date type variables to factor
data <- data %>% 
  mutate(
    PLANT_ID = factor(PLANT_ID),
    PRODUCTION_LOCATION = factor(PRODUCTION_LOCATION),
    MAINTENANCE_ACTIVITY_TYPE = factor(MAINTENANCE_ACTIVITY_TYPE),
    MAINTENANCE_TYPE_DESCRIPTION = factor(MAINTENANCE_TYPE_DESCRIPTION),
    FUNCTIONAL_AREA_NODE_1_MODIFIED = factor(FUNCTIONAL_AREA_NODE_1_MODIFIED),
    FUNCTIONAL_AREA_NODE_2_MODIFIED = factor(FUNCTIONAL_AREA_NODE_2_MODIFIED),
    FUNCTIONAL_AREA_NODE_3_MODIFIED = factor(FUNCTIONAL_AREA_NODE_3_MODIFIED),
    FUNCTIONAL_AREA_NODE_4_MODIFIED = factor(FUNCTIONAL_AREA_NODE_4_MODIFIED),
    FUNCTIONAL_AREA_NODE_5_MODIFIED = factor(FUNCTIONAL_AREA_NODE_5_MODIFIED),
    EQUIP_CAT_DESC = factor(EQUIP_CAT_DESC),
    MAINTENANCE_PLAN = factor(MAINTENANCE_PLAN),
    EQUIPMENT_ID = factor(EQUIPMENT_ID),
    MAINTENANCE_ITEM = factor(MAINTENANCE_ITEM)
  )

# Looking for the summary again for the new data

summary(data)

```


## Creating variables to prepare data for survival analysis

```{r}
# Create failure indicator (1 = failure, 0 = no failure)
data <- data %>% 
  mutate(failure = ifelse(MAINTENANCE_ACTIVITY_TYPE == "Unplanned", 1, 0))

# Sort data by FUNCTIONAL_AREA_NODE_2_MODIFIED and EXECUTION_START_DATE
data <- data %>% arrange(FUNCTIONAL_AREA_NODE_2_MODIFIED, EXECUTION_START_DATE)

# Calculate time_to_failure and time_since_installation
data <- data %>%
  group_by(FUNCTIONAL_AREA_NODE_2_MODIFIED) %>%
  mutate(
    time_to_failure = ifelse(failure == 1, as.numeric(difftime(EXECUTION_START_DATE, lag(EXECUTION_START_DATE), units = "days")), NA),
    time_since_installation = as.numeric(difftime(EXECUTION_START_DATE, EQUIP_VALID_FROM, units = "days"))
  ) %>%
  fill(time_to_failure, .direction = "down") %>%
  ungroup() %>%
  # Replace NA values with 0, then adjust time_to_failure to have a minimum of 1
  mutate(
    time_to_failure = replace_na(time_to_failure, 0),
    time_to_failure = ifelse(time_to_failure == 0, 1, time_to_failure),
    time_since_installation = replace_na(time_since_installation, 0)
  )

# Looking at the new variables by getting the summary of the data again
summary(data)

```


The summary above tells us that almost 90% of maintenance is unplanned and occurs due to failure in an equipment or it's part. The time since failure has a low mean of around 3 days since a lot of the Equipment ID's were imputed with 400029000 and hence the failure time for them are calculated as zero.

# Exploratory Data Visualizations

For the sake of showing data patterns through visualizations we are going to take out the 400029000 that we imputed. We will start with some univariate analysis of important variables and than will move to bivariate and multivariate analysis. 


```{r}
# Removing Outliers from the data set

data_clean <- data %>% 
  filter(ACTUAL_WORK_IN_MINUTES < 20000) %>% 
  mutate(failure = factor(failure)) %>% 
  filter(time_since_installation > 0)

# Checking the summary characteristics of the clean data
summary(data_clean)

# Histogram of Actual Work in Minutes

ggplot(data = data_clean,
       mapping = aes(x = ACTUAL_WORK_IN_MINUTES)) +
  geom_histogram() +
  labs(title = "Distribution of Actual Work Time in Minutes",
       x = "Actual Work Time (Minutes)", y = "Frequency")

```

```{r}

# Scatter plot between two continuous variable with target as color
data_clean %>% 
  ggplot(mapping = aes(x = time_since_installation, y = time_to_failure, color = failure)) +
  geom_point() +
  labs(title = "Scatterplot of time_since_failure vs time_since_installation by failure")

```

It can be seen from the scatter plot above that as the time since equipment is installed increases, the time between failures is more at the start and end of the graph meaning that the equipments are either performing without failure at the start when they are initially installed or at the end of their lifecycle probably due to good planned maintenance.

```{r}

# Bar Chart between MAINTENANCE_ACTIVITY_TYPE and PRODUCTION_LOCATION by failure

data_clean %>%
  ggplot(mapping = aes(x = PRODUCTION_LOCATION, fill = failure)) +
  geom_bar(position = "dodge") +
  facet_wrap(~ MAINTENANCE_ACTIVITY_TYPE) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.text.y = element_text(angle = 0)) +
  labs(title = "Distribution of MAINTENANCE_ACTIVITY_TYPE and PRODUCTION_LOCATION by failure",
       x = "PRODUCTION_LOCATION",
       y = "Count",
       fill = "failure")

```

The above bar charts show that MONZA had the most Planned Scheduled Maintenance and hence comparatively low Failure rate while COTA had little planned maintenance and hence it had the highest failure rate.

```{r}
# Heatmap between FUNCTIONAL_AREA_2 and failure

data_clean %>%
  count(FUNCTIONAL_AREA_NODE_2_MODIFIED, failure) %>%
  ggplot(mapping = aes(x = FUNCTIONAL_AREA_NODE_2_MODIFIED, y = failure)) +
  geom_tile(mapping = aes(fill = n)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.text.y = element_text(angle = 0)) +
  labs(title = "Heatmap between FUNCTIONAL_AREA_2 and failure")

```

From the heatmap above it can be seen that the highest number of failures occurred in CAN_LINE and the highest number of planned maintenance occurred in BTL_PET_LINE.


```{r}

# Boxplot of time_since_failure across Maintenance Activity Types
ggplot(data_clean, aes(x = EQUIP_CAT_DESC, y = time_to_failure)) +
  geom_boxplot() +
  labs(title = "Boxplot of Time Since Failure by Equipment Category Description", x = "Equipment Category", y = "Time Since Failure")


```

From the Box and Whiskers plot above it can be denoted that Machines have a higher time since failure than other equipment categories hence it can be suggested that equipments are a little more reliable than other equipment categories although it can be due to more outliers in the machine category and more data is required to show if the machines fail less often than other equipment categories.


```{r}
# Heatmap between FUNCTIONAL_AREA_4 and failure

data_clean %>%
  count(FUNCTIONAL_AREA_NODE_4_MODIFIED, failure) %>%
  ggplot(mapping = aes(x = FUNCTIONAL_AREA_NODE_4_MODIFIED, y = failure)) +
  geom_tile(mapping = aes(fill = n)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.text.y = element_text(angle = 0)) +
  labs(title = "Heatmap between FUNCTIONAL_AREA_4 and failure")

```

From the above heatmap it can be denoted that for Functional Area Node 4, the FILLER had the most planned scheduled maintenance but it is also the one that failed the most. Quite Alarming!


# Results, Findings and Conclusion of Exploratory Data Analysis

The exploratory data analysis (EDA) on Swire's dataset, containing over 1.4 million observations, revealed significant missing values in some key columns like "MAINTENANCE_ITEM" (90% missing) and "EQUIPMENT_ID" (80% missing). To proceed with the analysis, missing equipment IDs were imputed based on preceding values, assuming that a single Equipment ID may represent multiple components.

Nearly 90% of the maintenance was unplanned, suggesting heavy reliance on reactive maintenance. After doing data cleaning some patterns emerged showing that newly installed equipment tends to perform well without failures, and equipment with planned maintenance towards the end of its lifecycle also experienced fewer failures. Plants with more scheduled maintenance, like MONZA, had lower failure rates, while COTA, with less planned maintenance, had higher failure rates.

Additionally, specific equipment categories like CAN_LINE showed higher failure rates, while BTL_PET_LINE had more scheduled maintenance. A notable and alarming observation was that FILLER equipment in Functional Area Node 4, despite receiving the most planned maintenance, had the highest failure rate. This indicates a need to reassess the effectiveness of the current maintenance approach and maybe replace that machine since a high amount of preventive maintenance isn't enough. Overall, the analysis provides insights that could help improve preventive maintenance strategies and enhance equipment reliability and decrease the time since failure.


# Survival Analysis Preparation

## Identify the First Failure Event for Each FUNCTIONAL_AREA_NODE_2_MODIFIED

```{r}

failure_data <- data %>%
  filter(failure == 1) %>% 
  group_by(FUNCTIONAL_AREA_NODE_2_MODIFIED) %>% 
  summarise(
    Avg_time_to_failure = mean(time_to_failure, na.rm = TRUE),
    equipment_type = first(na.omit(EQUIPMENT_DESC)), # Selects first non-NA value
    plant_id = first(PLANT_ID),
    EQUIPMENT_ID = first(EQUIPMENT_ID),
    failure = 1
  )

head(failure_data)

```

The output provides the first few rows of failure_data, which includes:

Avg_time_to_failure: This is the average time until failure for equipment within each FUNCTIONAL_AREA_NODE_2_MODIFIED. For example, equipment in the "AIR SYSTEMS" area has an average time to failure of 1.36 days.
Equipment Details: The selected equipment_type, plant_id, and EQUIPMENT_ID represent the first encountered failure within each area, giving insights into the equipment type and plant where failures are frequent.
This output indicates which functional areas and specific equipment types may need more frequent or preventive maintenance due to shorter average times to failure.

## Identify Censored Equipments

Equipments without any failure events are considered censored.

```{r}
# Calculate censored data for equipment with no failures
censored_data <- data %>%
  group_by(FUNCTIONAL_AREA_NODE_2_MODIFIED) %>%
  filter((failure == 0)) %>% 
  summarise(
    Avg_time_to_failure = as.numeric(difftime(max(EXECUTION_START_DATE), first(EQUIP_VALID_FROM), units = "days")),
    equipment_type = first(na.omit(EQUIPMENT_DESC)),  # Selects first non-NA equipment type
    plant_id = first(PLANT_ID),
    EQUIPMENT_ID = first(EQUIPMENT_ID),
    failure = 0
  )

head(censored_data)

```

In censored_data, some rows show Avg_time_to_failure as NA, which suggests either missing date information or that the duration calculation didn't produce a valid result. These NA values may need investigation to confirm whether these equipment items truly have no time-to-failure information.

For equipment with valid Avg_time_to_failure values (like 2750 days for "AIR SYSTEMS" equipment DRYER_AIR_REFRIG), the data gives a measure of the equipment's operational duration without failure, useful for identifying potentially robust equipment.


## Combine Failure and Censored Data

```{r}
# Combine the datasets
survival_data <- bind_rows(failure_data, censored_data)

# Ensure no duplicates
survival_data <- survival_data %>% distinct(FUNCTIONAL_AREA_NODE_2_MODIFIED, .keep_all = TRUE)

```

The combination enables the survival analysis to consider both failure (event) and censored (non-failure) data, crucial for accurately modeling survival times in a maintenance context.

## Create Survival Object

```{r}
# Create survival object
surv_object <- Surv(time = survival_data$Avg_time_to_failure, event = survival_data$failure)

```

The surv_object is created using the Surv() function, with Avg_time_to_failure as the survival time and failure as the event indicator. This object is essential for fitting the survival model.

The survival object represents the core data structure for survival analysis, combining the time and event status for each functional area. This step prepares the data for Kaplan-Meier estimation and other survival analysis techniques.


# Survival Analysis

## Kaplan-Meier Estimator

```{r}
# Fit Kaplan-Meier survival model
km_fit <- survfit(surv_object ~ 1)

# Plot the survival curve
ggsurvplot(km_fit, data = survival_data, conf.int = TRUE, pval = TRUE,
           title = "Kaplan-Meier Survival Curve",
           xlab = "Time to Failure (Days)", ylab = "Survival Probability")

```

The Kaplan-Meier curve is a visual indicator of equipment reliability over time, revealing:

Potentially high-risk periods where survival drops sharply.
Differences in survival probabilities between failure-prone areas and more robust equipment setups, which is useful for prioritizing maintenance resources effectively.

Interpretation of Kaplan-Meier Survival Curve

Survival Probability: The y-axis shows the probability of equipment surviving without failure over time (days on the x-axis). A steeper decline in the curve indicates faster equipment failure, while a more gradual slope suggests greater equipment longevity.
Confidence Intervals: Shaded areas around the curve represent confidence intervals, offering a range within which we expect the true survival probabilities to fall.

## Survival Analysis by Equipment Type

```{r}
library(gridExtra)

# Ensure `equipment_type` is a factor and sort levels for consistency
survival_data <- survival_data %>%
  mutate(equipment_type = as.factor(equipment_type))

# Get unique equipment types and split into groups of 5-7
equipment_groups <- split(levels(survival_data$equipment_type), 
                          ceiling(seq_along(levels(survival_data$equipment_type)) / 7))

# Create a list to store plots
plot_list <- list()

# Loop through each group, fit survival curves, and create plots
for (i in seq_along(equipment_groups)) {
  
  # Filter survival data for the current group of equipment types
  group_data <- survival_data %>%
    filter(equipment_type %in% equipment_groups[[i]])
  
  # Create a survival object for the group
  surv_object_group <- Surv(time = group_data$Avg_time_to_failure, event = group_data$failure)
  
  # Fit Kaplan-Meier survival curves for the group
  km_fit_group <- survfit(surv_object_group ~ equipment_type, data = group_data)
  
  # Plot the survival curves for the group without p-value
  plot_list[[i]] <- ggsurvplot(
    km_fit_group, data = group_data, conf.int = TRUE, pval = FALSE,
    title = paste("Survival Curves for Equipment Types - Group", i),
    xlab = "Time to Failure (Days)", ylab = "Survival Probability",
    legend.title = "Equipment Type",
    legend = "right",
    ggtheme = theme_minimal(base_size = 10),
    font.legend = 8,
    palette = "Dark2"
  )
}

# Display all plots one by one
for (i in seq_along(plot_list)) {
  print(plot_list[[i]])
  Sys.sleep(1)  # Optional: pause for 1 second between plots for better readability
}

```


## Survival Analysis by Equipment_IDs

```{r}
# Filter the survival_data for relevant Equipment_IDs
filtered_data <- survival_data %>%
  filter(FUNCTIONAL_AREA_NODE_2_MODIFIED != "G812 SHOP / REPAIR AREA")

# Ensure `EQUIPMENT_ID` is a factor and sort levels for consistency
filtered_data <- filtered_data %>%
  mutate(EQUIPMENT_ID = as.factor(EQUIPMENT_ID))

# Define the number of Equipment_IDs per plot
num_ids_per_plot <- 5

# Create a list to store plots
plot_list_1 <- list()

# Split filtered data into chunks of 5 Equipment_IDs each
unique_ids <- unique(filtered_data$EQUIPMENT_ID)
id_chunks <- split(unique_ids, ceiling(seq_along(unique_ids) / num_ids_per_plot))

# Loop through each chunk, fit survival curves, and create plots
for (i in seq_along(id_chunks)) {
  
  # Filter survival data for the current chunk of Equipment_IDs
  group_data_1 <- filtered_data %>%
    filter(EQUIPMENT_ID %in% id_chunks[[i]])
  
  # Skip if group_data_1 is empty or has all missing Avg_time_to_failure or failure
  if (nrow(group_data_1) == 0 || all(is.na(group_data_1$Avg_time_to_failure)) || all(is.na(group_data_1$failure))) {
    next  # Skip this group and move to the next one
  }
  
  # Create a survival object for the group
  surv_object_group_1 <- Surv(time = group_data_1$Avg_time_to_failure, event = group_data_1$failure)
  
  # Fit Kaplan-Meier survival curves for the group
  km_fit_group_1 <- survfit(surv_object_group_1 ~ EQUIPMENT_ID, data = group_data_1)
  
  # Plot the survival curves for the group without p-value
  plot_list_1[[i]] <- ggsurvplot(
    km_fit_group_1, data = group_data_1, conf.int = TRUE, pval = FALSE,
    title = paste("Survival Curves for Equipment IDs - Group", i),
    xlab = "Time to Failure (Days)", ylab = "Survival Probability",
    legend.title = "Equipment IDs",
    legend = "right",
    ggtheme = theme_minimal(base_size = 10),
    font.legend = 8,
    palette = "Dark2"
  )
}

# Display all plots in plot_list_1, skipping any empty entries
for (i in seq_along(plot_list_1)) {
  if (!is.null(plot_list_1[[i]])) {
    print(plot_list_1[[i]])
  }
}

```



# Cox Proportional Hazards Model

## Build the Model

```{r}
# Fit Cox Proportional Hazards model
cox_model <- coxph(surv_object ~ equipment_type + plant_id, data = survival_data)

# Model summary
summary(cox_model)

```

Model Summary

The model is fitted using equipment_type and plant_id as predictors for the survival outcome. The model used 43 observations with 43 events (failures), while 24 observations were excluded due to missing data.

Likelihood Ratio, Wald, and Score Tests:
The Likelihood Ratio Test (p < 2e-16) and the Score Test (p = 5e-14) are statistically significant, indicating that at least one coefficient in the model is significantly associated with the survival time. However, the Wald Test (p = 1) suggests that the Wald statistic for this model doesn’t provide evidence of a global effect of the covariates, likely due to the large standard errors.

Coefficients (coef):

These values represent the log hazard ratios for each equipment_type and plant_id compared to a baseline. A negative coefficient suggests a protective effect (reducing hazard or risk of failure), while a positive coefficient indicates an increased hazard.
Some coefficients (e.g., equipment_typeCIP #1 PUMP - P901) have extreme values, which could indicate sparse data for these types, leading to large or unstable estimates.

Hazard Ratios (exp(coef)):

The hazard ratios (HRs) are the exponentiated coefficients (exp(coef)). For example, an HR of 4.548e-88 indicates an extremely low risk, while an HR of 2.463e+11 indicates an extremely high risk. However, the interpretability of these extreme values is limited, as they are likely the result of very small sample sizes or limited events for those categories.

Generally:
HR < 1: Suggests the equipment or plant has a lower hazard (decreased failure risk).
HR > 1: Suggests the equipment or plant has a higher hazard (increased failure risk).
Some of these values are so large that they might be unreliable due to overfitting or multicollinearity.

Statistical Significance (Pr(>|z|)):

Most predictors have high p-values (close to or above 0.05), indicating a lack of statistical significance. This is likely due to the large standard errors and may suggest limited or imprecise information in the data for certain equipment_type and plant_id categories.

In summary, while the Cox model output provides initial insights into which equipment types and plants are associated with higher or lower hazards, the instability of estimates suggests that refining the model (e.g., by grouping or using regularization) would improve interpretability and reliability.

## Check Proportional Hazards Assumption

```{r}
# Test proportional hazards assumption
cox_zph <- cox.zph(cox_model)
print(cox_zph)

# Plot Schoenfeld residuals
ggcoxzph(cox_zph)

```

The cox.zph(cox_model) function conducts a test of the proportional hazards assumption for each covariate in the model as well as for the global model. 

p-value:
A low p-value (typically < 0.05) indicates a violation of the proportional hazards assumption for that variable, suggesting the effect of the variable changes over time.
A high p-value (> 0.05) suggests the proportional hazards assumption is reasonably met for that variable.

It can be seen above that the Cox model assumptions are satisfied by equipment_type and the estimated hazard ratios are valid as constants over time for it. On the other hand these assumptions are not satisfied for plant_id and for the global model.

# Time Series Analysis of Failures

```{r}
# Number of failures over time
failure_over_time <- data %>%
  filter(failure == 1) %>%
  group_by(EXECUTION_START_DATE) %>%
  summarise(failures = n())

# Plot
ggplot(failure_over_time, aes(x = EXECUTION_START_DATE, y = failures)) +
  geom_line(color = "firebrick") +
  theme_minimal() +
  labs(title = "Failures Over Time",
       x = "Date", y = "Number of Failures")


```


# Correlation Analysis

```{r}
# Select numerical variables
numeric_vars <- Filter(is.numeric, data)

# Compute correlation matrix
cor_matrix <- cor(numeric_vars, use = "complete.obs")

# Display correlation matrix
print(cor_matrix)

```

# New Try - Survival Analysis #2

```{r}
# Checking NA values in Data

# Calculate count of NA and blank values for each column
missing_summary_2 <- data %>%
  summarise(across(everything(), 
                   ~sum(is.na(.)), 
                   .names = "NA_count_{col}")) %>%
  bind_rows(data %>%
              summarise(across(where(is.character), 
                               ~sum(. == "", na.rm = TRUE), 
                               .names = "blank_count_{col}")))

# Display the result
print(missing_summary_2)




```



